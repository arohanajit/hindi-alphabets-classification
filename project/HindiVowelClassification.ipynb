{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HindiVowelClassification.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arohanajit/hindi-alphabets-classification/blob/master/project/HindiVowelClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRb3CSg6nU_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm_notebook\n",
        "train_on_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHqK84msKjiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "os.environ['KAGGLE_USERNAME'\n",
        "] = \"xxxxxxxx\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'\n",
        "] = \"xxxxxxxxxxxxx\" # key from the json file\n",
        "!kaggle competitions download -c padhai-hindi-vowel-consonant-classification # api copied from kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo3d93IzEVZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!unzip test.zip\n",
        "!unzip train.zip\n",
        "!mkdir dataset\n",
        "!!mv train dataset/\n",
        "!mv test dataset/\n",
        "!rm train.zip\n",
        "!rm test.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euz2vievEXlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls dataset/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc67z13SAYMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#For converting the dataset to torchvision dataset format\n",
        "class VowelConsonantDataset(Dataset):\n",
        "    def __init__(self, file_path,train=True,transform=None):\n",
        "        self.transform = transform\n",
        "        self.file_path=file_path\n",
        "        self.train=train\n",
        "        self.file_names=[file for _,_,files in os.walk(self.file_path) for file in files]\n",
        "        self.len = len(self.file_names)\n",
        "        if self.train:\n",
        "            self.classes_mapping=self.get_classes()\n",
        "    def __len__(self):\n",
        "        return len(self.file_names)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        file_name=self.file_names[index]\n",
        "        image_data=self.pil_loader(self.file_path+\"/\"+file_name)\n",
        "        if self.transform:\n",
        "            image_data = self.transform(image_data)\n",
        "        if self.train:\n",
        "            file_name_splitted=file_name.split(\"_\")\n",
        "            Y1 = self.classes_mapping[file_name_splitted[0]]\n",
        "            Y2 = self.classes_mapping[file_name_splitted[1]]\n",
        "            z1,z2=torch.zeros(10),torch.zeros(10)\n",
        "            z1[Y1-10],z2[Y2]=1,1\n",
        "            label=torch.stack([z1,z2])\n",
        "\n",
        "            return image_data, label\n",
        "\n",
        "        else:\n",
        "            return image_data, file_name\n",
        "          \n",
        "    def pil_loader(self,path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "      \n",
        "    def get_classes(self):\n",
        "        classes=[]\n",
        "        for name in self.file_names:\n",
        "            name_splitted=name.split(\"_\")\n",
        "            classes.extend([name_splitted[0],name_splitted[1]])\n",
        "        classes=list(set(classes))\n",
        "        classes_mapping={}\n",
        "        for i,cl in enumerate(sorted(classes)):\n",
        "            classes_mapping[cl]=i\n",
        "        return classes_mapping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4OoFNIBrOIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform1 = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                            [0.229, 0.224, 0.225])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gCzRhMnmmIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_data=VowelConsonantDataset(\"../content/dataset/train\",train=True,transform=transform1)\n",
        "train_size = int(0.9 * len(full_data))\n",
        "test_size = len(full_data) - train_size\n",
        "\n",
        "train_data, validation_data = random_split(full_data, [train_size, test_size])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=True)\n",
        "test_data=VowelConsonantDataset(\"../content/dataset/test\",train=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64,shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucqr7l7dPEqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(train_data))\n",
        "print(len(validation_data))\n",
        "print(len(full_data))\n",
        "full_data.get_classes()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9_zSIbSmpcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wV9Wqf61OeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "print(images[0].shape,images[0].size(0))\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    img = np.transpose(np.squeeze(images[idx]))\n",
        "    ax.imshow(img)\n",
        "print(\"\\n\\n\\n\",torch.max(labels[:,0,:],1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gIv4kB1m-OC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vowel_model = torchvision.models.resnet50(pretrained=True)\n",
        "cons_model = torchvision.models.resnet50(pretrained=True)\n",
        "vowel_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaEHaeHu53fu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for param in vowel_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in cons_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diOwA6oS6Fc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vowel_model.fc = nn.Linear(2048,10,bias=True)\n",
        "cons_model.fc = nn.Linear(2048,10,bias=True)\n",
        "print(vowel_model.fc)\n",
        "print(cons_model.fc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QBjLMFuAD1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.linspace(0, 2 * np.pi, 400)\n",
        "y = np.sin(x ** 2)\n",
        "fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,5))\n",
        "ax1.plot(x)\n",
        "ax1.plot(y)\n",
        "ax1.legend(['abc','def'])\n",
        "ax2.plot(x)\n",
        "ax2.plot(y)\n",
        "ax2.legend(['abc','def'])\n",
        "ax3.plot(x)\n",
        "ax3.plot(y)\n",
        "ax3.legend(['abc','def'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ken0wfLYXmef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vowel_model.to(device)\n",
        "cons_model.to(device)\n",
        "loss_fn_v = nn.CrossEntropyLoss()\n",
        "loss_fn_c =  nn.CrossEntropyLoss()\n",
        "opt_V = optim.Adam(vowel_model.parameters())\n",
        "opt_C = optim.Adam(cons_model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuotocZa3ek6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    epochs = 45\n",
        "    total_trainloss = []\n",
        "    total_valloss = []\n",
        "    total_voweltrainacc = []\n",
        "    total_vowelvalacc = []\n",
        "    total_constrainacc = []\n",
        "    total_consvalacc = []\n",
        "    for i in tqdm_notebook(range(epochs)):\n",
        "        train_loss = 0\n",
        "        valid_loss = 0\n",
        "        totalval_train = 0\n",
        "        totalval_val = 0\n",
        "        acc_Vtrain = 0\n",
        "        acc_Ctrain = 0\n",
        "        acc_Vvalid = 0\n",
        "        acc_Cvalid = 0\n",
        "        vowel_model.train()\n",
        "        cons_model.train()\n",
        "        for image,label in tqdm_notebook(train_loader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            totalval_train+=64\n",
        "            opt_V.zero_grad()\n",
        "            opt_C.zero_grad()\n",
        "            out_V = vowel_model(image)\n",
        "            out_C = cons_model(image)\n",
        "            val_V,ind_V = torch.max(label[:,0,:],1)\n",
        "            val_C,ind_C = torch.max(label[:,1,:],1)\n",
        "            _,pred_Vtrain = torch.max(out_V,1)\n",
        "            _,pred_Ctrain = torch.max(out_C,1)\n",
        "            acc_Vtrain += (pred_Vtrain==ind_V).sum().item()\n",
        "            acc_Ctrain += (pred_Ctrain==ind_C).sum().item()\n",
        "            loss = loss_fn_v(out_V,ind_V)+loss_fn_c(out_C,ind_C)\n",
        "            train_loss += loss.item()*image.size(0)\n",
        "            loss.backward()\n",
        "            opt_V.step()\n",
        "            opt_C.step()\n",
        "            del image, label\n",
        "        \n",
        "        vowel_model.eval()\n",
        "        cons_model.eval()\n",
        "        for image,label in tqdm_notebook(validation_loader):\n",
        "            totalval_val += 64\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            V_out_V = vowel_model(image)\n",
        "            V_out_C = cons_model(image)\n",
        "            V_val_V,V_ind_V = torch.max(label[:,0,:],1)\n",
        "            V_val_C,V_ind_C = torch.max(label[:,1,:],1)\n",
        "            vloss = loss_fn_v(V_out_V,V_ind_V)+loss_fn_c(V_out_C,V_ind_C)\n",
        "            valid_loss += vloss.item()*image.size(0)\n",
        "            _,pred_Vvalid = torch.max(V_out_V,1)\n",
        "            _,pred_Cvalid = torch.max(V_out_C,1)\n",
        "            acc_Vvalid += (pred_Vvalid==V_ind_V).sum().item()\n",
        "            acc_Cvalid += (pred_Cvalid==V_ind_C).sum().item()\n",
        "        \n",
        "        train_loss /= len(train_loader)\n",
        "        valid_loss /= len(validation_loader)\n",
        "        V_acc_train = acc_Vtrain/totalval_train * 100\n",
        "        V_acc_valid = acc_Vvalid/totalval_val * 100\n",
        "        C_acc_train = acc_Ctrain/totalval_train * 100\n",
        "        C_acc_valid = acc_Cvalid/totalval_val * 100\n",
        "        total_trainloss.append(train_loss)\n",
        "        total_valloss.append(valid_loss)\n",
        "        total_voweltrainacc.append(V_acc_train)\n",
        "        total_vowelvalacc.append(V_acc_valid)\n",
        "        total_constrainacc.append(C_acc_train)\n",
        "        total_consvalacc.append(C_acc_valid)\n",
        "\n",
        "        print('Epoch: {} Train loss: {:.2f} Validation loss: {:.2f}'.format(i,train_loss,valid_loss))\n",
        "        print('Epoch: {} For vowels: Train accuracy: {:.2f} Validation accuracy: {:.2f}'.format(i,V_acc_train,V_acc_valid))\n",
        "        print('Epoch: {} For consonants: Train accuracy: {:.2f} Validation accuracy: {:.2f}'.format(i,C_acc_train,C_acc_valid))\n",
        "\n",
        "    torch.save(vowel_model.state_dict(), \"best_model_V.pth\")\n",
        "    torch.save(cons_model.state_dict(), \"best_model_C.pth\")\n",
        "    del out_V, out_C\n",
        "    torch.cuda.empty_cache()\n",
        "finally:\n",
        "    fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(20,5))\n",
        "    ax1.plot(total_trainloss)\n",
        "    ax1.plot(total_valloss)\n",
        "    ax1.legend(['Train Loss','Validation Loss'])\n",
        "    ax1.set_title('Loss')\n",
        "    ax2.plot(total_voweltrainacc)\n",
        "    ax2.plot(total_constrainacc)\n",
        "    ax2.legend(['Vowels','Consonants'])\n",
        "    ax2.set_title('Training Accuracy')\n",
        "    ax3.plot(total_vowelvalacc)\n",
        "    ax3.plot(total_consvalacc)\n",
        "    ax3.legend(['Vowels','Consonants'])\n",
        "    ax3.set_title('Validation Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asi-OI8K5OBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vowel_model.load_state_dict(torch.load(\"best_model_V.pth\"))\n",
        "vowel_model.to(device)\n",
        "vowel_model.eval()\n",
        "\n",
        "cons_model.load_state_dict(torch.load(\"best_model_C.pth\"))\n",
        "cons_model.to(device)\n",
        "cons_model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mruo8zsZMT3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total=0\n",
        "v=0\n",
        "c=0\n",
        "for data in tqdm_notebook(validation_loader,total=len(validation_loader),unit='batch'):\n",
        "    images,labels = data\n",
        "    images,labels = images.to(device),labels.to(device)\n",
        "    _,out_v = torch.max(vowel_model(images),1)\n",
        "    _,out_c = torch.max(cons_model(images),1)\n",
        "    _,lab1 = torch.max(labels[:,0,:],1)\n",
        "    _,lab2 = torch.max(labels[:,1,:],1)\n",
        "    total += 64\n",
        "    v += (out_v==lab1).sum().item()\n",
        "    c += (out_c==lab2).sum().item()\n",
        "print('total images:',total)\n",
        "print('correct vowels predictions:',v)\n",
        "print('correct consonants predictions:',c)\n",
        "print('Vowel Accuracy: ',(v/total)*100, '%')\n",
        "print('Consonants Accuracy: ',(c/total)*100,'%')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN8sPrq4MEPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}